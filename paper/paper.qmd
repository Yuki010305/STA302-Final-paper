---
title: "Zillow House Price Predict Analysis"
author: 
  - "Yaning Jin"
thanks: "Code and data are available at: https://github.com/Yuki010305/Zillow-House-Price-Predict-Analysis.git."
date: today
date-format: long
abstract: "This project is interested in the factors affecting housing prices on zillow. After exploratory analysis of the data, a multiple linear regression was constructed, and a log transformation was performed to build a model with better performance. The transformed housing factors can explain more than 80% of the changes in housing prices. However, the existence of residuals in the model does not fully meet the constraints of the normality assumption, so in the future, data processing, feature engineering and model improvement are needed to study the factors affecting Zillow's housing prices.."
format: pdf
number-sections: true
bibliography: references.bib
link-citations: true
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(gridExtra)
library(ggplot2)
library(stargazer)
library(xtable)
library(modelsummary)
library(kableExtra)
library(tinytex)

```


\tableofcontents


# Introduction

One of a nation's key industries is housing, and the health of this sector influences the degree of economic growth of the nation. A person's home is frequently the most expensive item in their life. Zillow is an online real estate database provider that assesses property values and offers details on the houses you're interested in [@loukissas2018all]. The Zestimate prediction system was developed by Zillow. It is based on a data collection of hundreds of millions of homes and provides a preliminary estimate of a property's worth by combining a particular algorithm with the features of each home and the state of the market[@rolli2020zillow]. To improve the accuracy of the Zestimate system and provide people with a more trustworthy way to estimate home prices, Zillow launched the Zillow Awards competition in 2017. Our goal in this project is not to participate in a competition to minimize the logarithmic error between the estimated house price and the actual selling price. The main purpose of our exploration in this project is to explore the factors that affect Zillow home sales prices and how they affect home sales prices. Characteristics of the homes we select include number of bathrooms, number of bedrooms, square footage, number of rooms, year, tax value, land tax value, etc. The estimated model is 

$$\text{log(price)}=\beta_0+\beta_1\text{bathrooms}+\beta_2\text{log(squarefootage)}+...+\beta_6\text{taxValue}$$
For some variables we will also do some log transformations.


# Data {#sec-data}

```{r}
#| label: Zillow
#| fig-cap: Zillow house
#| echo: false

zillow <- read.csv("../data/analysis_data/zillow_price.csv")

```

## Raw Data

In this research, we examine zillow data sourced using the zillbowR library [@rolli2020zillow]. The dataset encompasses 90,275 records (89,499 records after cleaning), focusing on specific variables: bathroom number, bedroom number, year built, square feet, room number, tax value, etc. The apartments were built in a wide range of years, from as early as 1885 to as recently as 2015.


The Raw data collects 60 features of the house, and data quality is not high, there are many features with many null values, so 8 high-quality variables are selected from the variables for modeling and prediction.


## Data Analysis tools

R [@citeR], a potent open-source statistical programming language, was used to analyze the data. To improve the effectiveness of our data operations, we used a collection of R packages from the tidyverse [@citetidyverse], which is a collection of tools created for data science. The `dplyr` package [@rDplyr] offered a consistent collection of verbs that aid in filtering, summarizing, and organizing the dataset, while the `ggplot2` package [@rGgplot2] made it easier to create complex visualizations. Because of its quick and user-friendly data reading capabilities, the `readr` package [@rReadr] was used.  `Knitr` [@rKnitr] handled report production dynamically, allowing R code to be included into this document. `kableExtra`[@R-kableExtra] was also used to create visually appealing and editable tables, which improved the way our results were presented. 




```{r}
#| label: Zillow select variable
#| fig-cap: Zillow house save
#| echo: false

numeric_cols <- c('bathroomcnt',
                  'bedroomcnt',
                  'calculatedfinishedsquarefeet',
                  'roomcnt',
                  'yearbuilt',
                  'taxvaluedollarcnt',
                  'landtaxvaluedollarcnt',
                  'price')

zillowtrim <- zillow[, numeric_cols]



zillowtrim <- na.omit(zillowtrim)

write.csv(zillowtrim,"../data/analysis_data/zillowtrim_analysis.csv",row.names = FALSE)

```

## Variable Description {#sec-variable}

|Variable|Description|
|---|---|  
|bathroomcnt|Number of bathrooms in home|   
|bedroomcnt|Number of bedrooms in home|  
|calculatedfinishedsquarefeet|Total finished living area of the home|  
|roomcnt|Total number of rooms|  
|yearbuilt|The Year the principal residence was built|  
|taxvaluedollarcnt|The total tax assessed value of the parcel|  
|landtaxvaluedollarcnt|The assessed value of the land area| 
|price|price| 


## Measurement

In this study, we leveraged data from the Zillow dataset, accessed through the Zillow API. The Zillow dataset provides comprehensive information on residential properties, enabling analyzes focused on housing market dynamics, property valuation, and spatial characteristics.

The Zillow dataset encompasses various attributes crucial for understanding residential properties' characteristics and market behavior. Specifically:

bathroomcnt: Indicates the number of bathrooms present in each residential property, influencing its utility and convenience for occupants.
bedroomcnt: Represents the count of bedrooms within each residential unit, influencing its suitability for different household compositions.
calculatedfinishedsquarefeet: Quantifies the total finished living area of the home, reflecting its size and spatial configuration.
roomcnt: Denotes the total number of rooms in the residential unit, encompassing living spaces, bedrooms, and other functional areas, providing insights into the property's layout.
yearbuilt: Specifies the year in which the principal residence was constructed, offering insights into the age distribution of properties within the dataset.
taxvaluedollarcnt: Indicates the total tax-assessed value of the parcel, serving as a key indicator of property valuation for taxation purposes.
landtaxvaluedollarcnt: Reflects the assessed value of the land area associated with each residential parcel, contributing to the overall property valuation.
price: Represents the price of the residential property, serving as a fundamental metric for market valuation and investment analysis.
By analyzing these attributes, researchers can gain valuable insights into housing market trends, property valuation factors, and the impact of various features on real estate prices within the studied area.

To ensure the dataset's suitability for analysis, we performed several preprocessing steps. Select high-quality variables with few missing values and remove missing values. Additionally, we implemented data cleaning procedures to exclude observations with missing values, ensuring the completeness and reliability of our dataset for subsequent analyses.

For detailed descriptions of each variable, please refer to the table provided in Section @sec-variable.





# Exploratory Data Analysis {#sec-exploration}


```{r}
#| label: summary
#| fig-cap: summary numeric
#| echo: false
#summary(zillowtrim[,-c(1,2,4,5)])

#kableExtra::kable(t(zillowtrim[,-c(1,2,4,5)]), format="latex")
```


```{r,echo=FALSE,eval=FALSE}
#| label: air
#| fig-cap: central air table
#| echo: false
### Categorical table
table(zillowtrim$yearbuilt)
```



```{r,echo=FALSE,eval=FALSE}
#| label: bedroomcnt
#| fig-cap: bedroomcnt table
#| echo: false
table(zillowtrim$bedroomcnt)
prop.table(table(zillowtrim$bedroomcnt))
```

```{r }
#| label: fig-histprice
#| fig-cap: sale price and log sale price hist
#| echo: false
par(mfrow=c(1,2))
hist(zillowtrim$price, breaks=10, xlab="Sale Price",main="Sale price",col="red") 
hist(log(zillowtrim$price), breaks=10, xlab="log-transform Sale Price",main="Sale price",col="red") 
```
@fig-histprice, house sales prices are right-skewed data. When building the model, we need to log-transform the house sales prices to make them conform to the normal distribution.

```{r}
#| label: histbox
#| fig-cap: hists
#| echo: false

#par(mfrow=c(1,2))

#hist(zillowtrim$calculatedfinishedsquarefeet, breaks=10,main="", xlab="square feet")
#boxplot(zillowtrim$taxvaluedollarcnt, xlab="tax value")
#hist(zillowtrim$bedroomcnt, breaks=10, main="",xlab="bedroom")

#par(mfrow=c(1,2))
#boxplot(zillowtrim$calculatedfinishedsquarefeet,xlab="square feet")
#boxplot(zillowtrim$roomcnt, xlab="room")

#par(mfrow=c(1,2))
#boxplot(zillowtrim$taxvaluedollarcnt, xlab="tax value")
#boxplot(zillowtrim$landtaxvaluedollarcnt, xlab="land tax value")
```



```{r,warning=FALSE,message=FALSE}
#| label: point
#| fig-cap: points
#| echo: false
#a=ggplot(data=zillowtrim, aes(x=bathroomcnt, y=price)) + 
#  geom_point() + 
#  geom_smooth(method = lm, se = FALSE,size=1.5,color="red") + 
#  labs(x = 'bathroom', y='Sale Price')+
#  theme_bw()


#b=ggplot(data=zillowtrim, aes(x=taxvaluedollarcnt, y=price)) + 
#  geom_point() + 
#  geom_smooth(method = lm, se = FALSE,size=1.5) + 
#  labs(x = 'tax value', y='Sale Price')+
#  theme_bw()

#grid.arrange(a,b, nrow=1)
```


```{r,warning=FALSE,message=FALSE}
#| label: fig-pointstwo
#| fig-cap: Bivariate visualization
#| echo: false
a=ggplot(data=zillowtrim, aes(x=calculatedfinishedsquarefeet, y=price)) + 
  geom_point() + 
  geom_smooth(method = lm, se = FALSE,size=1.5,color="red") + 
  labs(x = 'square feet', y='Sale Price')+
  theme_bw()


b=ggplot(data=zillowtrim, aes(x=taxvaluedollarcnt, y=price)) + 
  geom_point() + 
  geom_smooth(method = lm, se = FALSE,size=1.5) + 
  labs(x = 'tax value', y='Sale Price')+
  theme_bw()

grid.arrange(a,b, nrow=1)
```

@fig-pointstwo shows that tax value has a certain positive impact on sale price, the positive correlation between tax value and sale price is very strong. @fig-pointstwo also shows that square feet has a certain positive impact on sale price, the positive correlation between square feet and sale price is very strong.


# Model

## Model set-up

After data processing, the data set is a clean data set with 2929 observations and 10 house characteristic variables. In order to evaluate the performance of the model, we randomly split the analysis data set into a test set and a training set in a ratio of 75%:25%.

```{r}
#| label: spilt
#| fig-cap: spilt data
#| echo: false
#| 
set.seed(123456)
rows <- sample(1:round(0.75*nrow(zillowtrim)), replace=FALSE) 
train<- zillowtrim[rows,]
test=zillowtrim[-rows,]
```

The first model we build is the full model. We then improve the model by removing insignificant variables.

```{r}
#| label: full
#| fig-cap: full model
#| echo: false
#| 
### full model
full = lm(log(price) ~ ., data = train)
#summary(full)
```



```{r}
#| label: sub
#| fig-cap: sub model
#| echo: false
#| 
log_ml = lm(log(price) ~ bathroomcnt+bedroomcnt+log(calculatedfinishedsquarefeet)+roomcnt+yearbuilt+log(taxvaluedollarcnt)+log(landtaxvaluedollarcnt), data = train)
#summary(log_ml)
```

```{r}
#| label: savebestmodel
#| fig-cap: save besr model
#| echo: false
#| 
saveRDS(log_ml , "../models/best_model.rds")
```



```{r}
#| label: plot1
#| fig-cap: plot model
#| echo: false
#par(mfrow=c(2,2))
#plot(log_ml)
```

The residual test found that both ends of the QQ graph deviated greatly from the straight line and were affected by special points such as outliers and leverage points. So, in order to further improve the performance of model fitting, we will delete special points from the training set.

```{r}
#| label: outlier
#| fig-cap: remove outliers
#| echo: false

# determine whether there are leverage points
n <- nrow(train)
p <- length(coef(log_ml))-1
# leverage cutoff
h_cut <- 2*(p+1)/n 
off = which(hatvalues(log_ml) > h_cut)
new_train = train[-off,]
```

After deleting special points such as level points, a new model was refitted.


```{r}
#| label: model2
#| fig-cap: create model 2
#| echo: false
#| 
ml_new = lm(log(price) ~ bathroomcnt+bedroomcnt+log(calculatedfinishedsquarefeet)+roomcnt+yearbuilt+log(taxvaluedollarcnt)+log(landtaxvaluedollarcnt), data = new_train)
#summary(ml_new)
#latex_code <- stargazer(full, log_ml, ml_new,type = "latex", title = "Regression Results" ,align = TRUE)
#cat(latex_code, file = "regression_results.tex")
#xelatex("regression_results.tex")
```




```{r}
#| label: savemodel
#| fig-cap: save model
#| echo: false
#| 
saveRDS(ml_new, "../models/model.rds")
```

### Model justification

```{r}
#| label: plotmodel2
#| fig-cap: plot model 2
#| echo: false
#plot(ml_new)
```


#### A1:Linearity of the Relationship

```{r}
#| label: fig-plothat
#| fig-cap: Y versus Y-hat 
#| echo: false
#| 
plot(new_train$price ~ exp(fitted(ml_new)), main="Y versus Y-hat", xlab="Y-hat", ylab="Y")
abline(a = 0, b = 1)
lines(lowess(new_train$price ~ exp(fitted(ml_new))), lty=2)
```

@fig-plothat fits the relationship between the predicted value and the actual value. You can see that these points are almost on or close to the line, so we can say that a linear relationship is satisfied.

#### A2.Covariance of Errors

```{r}
#| label: fig-plotresidual
#| fig-cap: Residual Plot
#| echo: false
#| 
plot(ml_new, which=1)

```

The residual plot @fig-plotresidual does not show any pattern, we can conclude that the residual terms are independently distributed across different predicted value ranges, consistent with the independence assumption of linear regression analysis.

#### A3.Common Error variance  

The residual plot @fig-plotresidual is not a uniform distribution. As the predicted value increases, the variance decreases, so the assumption of constant variance is not completely established.

#### A4. Normality of Error

```{r}
#| label: fig-plotqq
#| fig-cap: Normal Q-Q plot
#| echo: false
qqnorm(resid(ml_new))
qqline(resid(ml_new))
```

There are obvious deviations at both ends of the QQ graph @fig-plotqq, which may indicate that the data does not conform to the normal distribution in this area.


# Results

Estimating the value of a home is a common difficulty. Therefore, a lot of work has already been completed. Lee [@lee2016software] make an effort to develop multivariate regression models based on house datasets and assess the models using maximum information coefficient statistics based on anticipated values and home prices.  When a moderate to big data sample size is employed, Nghiep et al. [@nguyen2001predicting] examined two methods: multiple regression analysis (MRA) and artificial neural networks (ANN). Based on the prediction performance, ANN performs better than MRA. An artificial neural network (ANN) model was created after Limsombunchai [@limsombunchao2004house
] that ANNs are more accurate in predicting property values than hedonic regression models.  


The final model is:  

$$
\begin{aligned}
log(price) &=2.7-0.007bathroomcnt -0.006 bedroomcnt -0.2 log(squarefeet) \\ 
    &-0.015 roomcnt -0.00025 yearbuilt + 0.67log(taxvaluedollarcnt) -0.055 log(landtaxvaluedollarcnt)
\end{aligned}
$$

Adjusted R-squared is 0.8082, indicating that the model can explain approximately 80.83% of the variance of the target variable (logarithm of house prices).


The final model shows that the number of bathrooms, bedrooms, rooms, year taxes and fees of the building, square feet, etc. all have a significant impact on Zillowâ€™s housing prices. For every additional bathroom, the house price increases by 1.017% per unit. For every additional bedroom, the house price actually decreases by 0.01%. In addition, the newer the building is, the housing prices actually decrease. For every 1% unit increase in taxes and fees, housing prices increase by 0.67% unit.These findings explain the relationship between house-related attributes and house prices. 


# Discussion

## Discussion

**Model Interpretability and Generalization**
While the developed model provides valuable insights into the factors influencing housing prices, its interpretability is hindered by the transformation of data and the exclusion of certain categorical variables due to memory and time constraints. To address this, future research could explore methods to maintain model interpretability while optimizing predictive performance. Techniques such as feature selection and dimensionality reduction may help streamline the model without sacrificing interpretability.

**Addressing Residual Normality Assumption**
The observed violations of the normality assumption in the model's residuals raise concerns regarding its robustness and generalizability. To mitigate this issue, researchers could explore alternative modeling approaches or employ robust regression techniques that are less sensitive to deviations from normality. Additionally, conducting thorough diagnostic checks and sensitivity analyses can help identify potential sources of bias and improve the model's reliability.

**Enhancing Model Performance Through Cross-Validation**
The suggestion to partition training datasets into smaller subsets and perform cross-validation before generating test files holds promise for improving model performance and generalization. By systematically evaluating the model's performance across different subsets of data, researchers can gain insights into its stability and identify areas for refinement. Implementing rigorous cross-validation protocols can also enhance the model's credibility and ensure its applicability across diverse datasets.

**Future Directions in Feature Engineering**
Future iterations of the model could benefit from more sophisticated feature engineering techniques aimed at capturing the nuanced relationships between housing attributes and prices. Exploring advanced feature transformation methods, such as polynomial features or interaction terms, may help uncover hidden patterns and improve the model's predictive accuracy. Additionally, incorporating domain knowledge and expert insights into the feature selection process can enrich the model's explanatory power and enhance its utility for real-world applications.

**Handling Missing Data**
Addressing missing data is critical for ensuring the reliability and validity of the model's predictions. Researchers should carefully consider imputation strategies and explore techniques for incorporating missing variables as predictors in the model. By systematically addressing missing data issues, researchers can enhance the model's robustness and improve its performance on new datasets.

In summary, while the developed model provides valuable insights into the factors influencing housing prices, there are several avenues for future research to enhance its interpretability, robustness, and predictive accuracy. By addressing these challenges and leveraging advanced modeling techniques, researchers can develop more reliable and actionable models for estimating housing values.



## Weaknesses and next steps

However, the residuals of our model do not fully satisfy the normality assumption. Such violations may reduce the model's predictive accuracy on new data sets. Another limitation of this model is that transforming the data makes the model less interpretable.

To start making these models better in the future, divide each training dataset into smaller training tests and perform some cross-validation before generating the test files. This might enhance performance, or at the very least increase the predictability of the model's output. Although most categorical variables cannot be included in the model because of memory and time restrictions when running the model, the model may be enhanced with improved feature engineering. These variables can be included in the model by being divided into smaller groups. Making some interactive words is an additional thought. It is crucial to classify missing variables as predictors once all missing values have been imputed, as was previously noted.


\newpage

\appendix



\newpage

# References



